{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac572a-267e-4828-ab7a-1c52e30e1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@bchaipats/imrag-12f119c4ee0c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5ca87-3d64-4c2d-afca-d7eb09ce1bc9",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Enable the following APIs\n",
    "* [Claude 3 Haiku API](https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-3-haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e9ef3-2745-4b15-9796-5b02161fb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# IMGS_DIR = Path(\"/your/image/directory/\")\n",
    "# ds = ray.data.from_items([{\"path\": path.as_posix()} for path in IMGS_DIR.rglob(\"*.jpg\") if not path.is_dir()])\n",
    "# print(f\"{ds.count()} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcfabf-df18-4180-8322-70146658fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U -q 'anthropic[vertex]'\n",
    "! pip install -q -U streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587241c-920a-4343-82ff-7119f9a30646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "import re\n",
    "\n",
    "UNIQUE_PREFIX = socket.gethostname()\n",
    "UNIQUE_PREFIX = re.sub('[^A-Za-z0-9]+', '', UNIQUE_PREFIX)\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "# The region you want to launch jobs in.\n",
    "LOCATION = REGION = \"us-central1\" # \"us-central1\" # or \"europe-west4\"\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output. Fill it without the 'gs://' prefix.\n",
    "GCS_BUCKET = f\"{PROJECT_ID}-{UNIQUE_PREFIX}\"  # @param {type:\"string\"} \n",
    "BUCKET_URI = f\"gs://{GCS_BUCKET}\"  # @param {type:\"string\"}\n",
    "\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = !(gcloud config get-value core/account)  # @param {type:\"string\"}\n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "\n",
    "! gcloud storage buckets create {BUCKET_URI} --project={PROJECT_ID} --location={REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d160940-fccd-4a01-ae38-acf8bbd9f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import streamlit as st\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "#PROJECT_ID = os.environ.get(\"GCP_PROJECT\")  # Your Google Cloud Project ID\n",
    "\n",
    "#LOCATION = os.environ.get(\"GCP_REGION\")  # Your Google Cloud Project Region\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "\n",
    "# @st.cache_resource\n",
    "def get_model():\n",
    "    generation_model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
    "    return generation_model\n",
    "\n",
    "\n",
    "def get_text_generation(prompt=\"\", **parameters):\n",
    "    generation_model = get_model()\n",
    "    response = generation_model.predict(prompt=prompt, **parameters)\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e25bc3-9ed2-4993-940b-e8f0cdb3cca5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d167204-391e-4a6c-bc30-0b14ca1d1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "\n",
    "# input_prompt = \"\"\"can you give me details of paracetamol\"\"\"\n",
    "\n",
    "def generate(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-ultra\")\n",
    "    responses = model.generate_content(\n",
    "        input_prompt ,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32\n",
    "    },\n",
    "        safety_settings=[],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    all_response  = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # print(response.text, end=\"\")\n",
    "        all_response.append(response.text)\n",
    "    \n",
    "    # print (all_response)\n",
    "    \n",
    "    return(\" \".join(all_response))\n",
    "    \n",
    "\n",
    "def generate_pro(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    responses = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1\n",
    "    },stream=True,)\n",
    "    \n",
    "    all_response  = []\n",
    "    \n",
    "    for response in responses:\n",
    "        all_response.append(response.text)\n",
    "    \n",
    "    # print (all_response)\n",
    "    \n",
    "    return(\" \".join(all_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19823354-6d4f-4651-955b-ab7294953a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": f\"{LOCATION}-autopush-aiplatform.sandbox.googleapis.com\"}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "aiplatform_client = aiplatform.gapic.PredictionServiceClient(\n",
    "    client_options=client_options\n",
    ")\n",
    "\n",
    "def generate_medlpalm(input_prompt):\n",
    "    instance_dict = {\n",
    "        \"content\": input_prompt\n",
    "    }\n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    parameters_dict = {\n",
    "        \"maxOutputTokens\": 1024,\n",
    "        \"temperature\": 0.2,\n",
    "        \"topP\": 0.8,\n",
    "        \"topK\": 40\n",
    "    }\n",
    "    endpoint=f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/medpalm2@latest\"\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    response = aiplatform_client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters,\n",
    "    )\n",
    "    print(\"response\")\n",
    "    \n",
    "    predictions = response.predictions\n",
    "    all_response  = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        # print(\" prediction:\", dict(prediction)[\"content\"])\n",
    "        all_response.append(dict(prediction)[\"content\"])\n",
    "        \n",
    "    return(\" \".join(all_response))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038e5c4-02a2-4c34-bd21-157458bf67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "# # The AI Platform services require regional API endpoints.\n",
    "# client_options = {\"api_endpoint\": f\"{LOCATION}-autopush-aiplatform.sandbox.googleapis.com\"}\n",
    "# # Initialize client that will be used to create and send requests.\n",
    "# # This client only needs to be created once, and can be reused for multiple requests.\n",
    "# aiplatform_client = aiplatform.gapic.PredictionServiceClient(\n",
    "#     client_options=client_options\n",
    "# )\n",
    "\n",
    "def generate_medllms_v1(input_prompt):\n",
    "\n",
    "    instance_dict = {\n",
    "        \"content\": input_prompt\n",
    "    }\n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    parameters_dict = {\n",
    "        \"candidateCount\": 1,\n",
    "        \"maxOutputTokens\": 1024,\n",
    "        \"temperature\": 0.2,\n",
    "        \"topP\": 0.8,\n",
    "        \"topK\": 40\n",
    "    }\n",
    "    endpoint=f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/medlm-large\"\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    response = aiplatform_client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters,\n",
    "    )\n",
    "    print(\"response\")\n",
    "    predictions = response.predictions\n",
    "\n",
    "\n",
    "    all_response  = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        # print(\" prediction:\", dict(prediction))\n",
    "        # print(\" two\" ,  dict(prediction)[\"content\"] )\n",
    "        all_response.append(dict(prediction)[\"content\"])\n",
    "        \n",
    "    return(\" \".join(all_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474b7bc-6070-4f9f-8531-4c53437329dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 1,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "def generate_palm_unicorn_v1(input_prompt):\n",
    "    \n",
    "    model = TextGenerationModel.from_pretrained(\"text-unicorn@001\")\n",
    "\n",
    "    response = model.predict(\n",
    "        input_prompt,\n",
    "        **parameters\n",
    "    )\n",
    "    print(f\"Response from Model: {response.text}\")\n",
    "    \n",
    "    return(response.text)\n",
    "\n",
    "def generate_palm_bison32k(input_prompt):\n",
    "    \n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison-32k\")\n",
    "\n",
    "    response = model.predict(\n",
    "        input_prompt,\n",
    "        **parameters\n",
    "    )\n",
    "    print(f\"Response from Model: {response.text}\")\n",
    "    \n",
    "    return(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7815d-efa0-49d9-87a6-3fcdacd1c973",
   "metadata": {},
   "source": [
    "### Anthropic model now included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740fdde-d723-4910-8e5c-41aec7733f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from anthropic import AnthropicVertex\n",
    "\n",
    "AnthropicVertex_client = AnthropicVertex(region=LOCATION, project_id=PROJECT_ID)\n",
    "\n",
    "def generate_anthropic_response(input_prompt):\n",
    "  \"\"\"Generates a text response using the AnthropicVertex client.\n",
    "\n",
    "  Args:\n",
    "    input_prompt: The text prompt for the model.\n",
    "\n",
    "  Returns:\n",
    "    The generated text response.\n",
    "  \"\"\"\n",
    "  message = AnthropicVertex_client.messages.create(\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_prompt,\n",
    "      }\n",
    "    ],\n",
    "    model=\"claude-3-haiku@20240307\",  # or any other desired model\n",
    "  )\n",
    "\n",
    "  response_text = message.model_dump_json(indent=2)  # Accessing response\n",
    "  print(f\"Response from Model: {response_text}\")\n",
    "\n",
    "  return response_text \n",
    "\n",
    "def generate_anthropic_response_sonnet(input_prompt):\n",
    "  \"\"\"Generates a text response using the AnthropicVertex client.\n",
    "\n",
    "  Args:\n",
    "    input_prompt: The text prompt for the model.\n",
    "\n",
    "  Returns:\n",
    "    The generated text response.\n",
    "  \"\"\"\n",
    "  message = AnthropicVertex_client.messages.create(\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_prompt,\n",
    "      }\n",
    "    ],\n",
    "    # model=\"claude-3-haiku@20240307\",  # or any other desired model\n",
    "    model = \"claude-3-sonnet@20240229\" \n",
    "  )\n",
    "\n",
    "  response_text = message.model_dump_json(indent=2)  # Accessing response\n",
    "  print(f\"Response from Model: {response_text}\")\n",
    "\n",
    "  return response_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244aaa4-df39-403a-9442-b266bdbe62ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_prompt = \"What are the symptoms of influenza?\" \n",
    "\n",
    "# x = generate_pro(input_prompt)\n",
    "# generate_medllms_v1(input_prompt)\n",
    "# generate_palm_unicorn_v1(input_prompt)\n",
    "# input_prompt = \"What are the symptoms of influenza?\" \n",
    "# generate_medlpalm(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14778d8-a95a-4b04-9388-9325ec7a2e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1bdb2-9a61-4e42-b6f6-5f10922ef5b0",
   "metadata": {},
   "source": [
    "### First file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0a795-ce43-4501-80cb-2a13f4db9da6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = \"./data/Anaes 16 Clinical scenarios RAG response.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df['combine_prompt'] = df['System Prompts'] + ' ' +df['RAG Results'] + ' '+ df['User Question']\n",
    "\n",
    "\n",
    "# print(df['System Prompts'], df['RAG Results'] ,df['User Question'] )\n",
    "# print(selected_column[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c5884-8516-4d04-a3ff-59dcaa8f8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_medlpalm(df.loc[0 ,'combine_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c82c7f-1ff6-49cd-877e-64c2b420cdc2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(df)):\n",
    "\n",
    "    \n",
    "    if i<=100:\n",
    "        # df['Gemini_ultra_model_output'][i] = generate(df['combine_prompt'][i])\n",
    "        print(\"iteration #\", i)\n",
    "\n",
    "    df.loc[i, \"Gemini_ultra_model_output\"] = generate_pro(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"Gemini_ultra_model_output_v1\"] = generate_pro(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"Gemini_ultra_model_output_v2\"] = generate_pro(df.loc[i,'combine_prompt'])\n",
    "    \n",
    "    df.loc[i, \"Anthropic_haiko_model_output\"] = generate_anthropic_response(df.loc[i,'combine_prompt'])\n",
    "    \n",
    "    #df.loc[i, \"medLM_model_output\"] = generate_medllms_v1(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"medpalm_model_output\"] = generate_medlpalm(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"palm_bison32k_output\"] = generate_palm_bison32k(df.loc[i,'combine_prompt'])\n",
    "\n",
    "# generate_medllms_v1(input_prompt)\n",
    "# generate_palm_unicorn_v1(input_prompt)\n",
    "# input_prompt = \"What are the symptoms of influenza?\" \n",
    "# generate_medlpalm(input_prompt)    \n",
    "    \n",
    "# print( \"/n output here ::\" , df['Gemini_ultra_model_output'][i])\n",
    "# df = df.assign(Gemini_ultra_model_output=generate(df.combine_prompt))\n",
    "# df['combine_prompt'].head(3)\n",
    "\n",
    "# df['Gemini_ultra_model_output'].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363111b5-15ff-4934-ac79-f309d880f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f75a2b-4cc8-42fc-b7c3-c59cd9666168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df columnsname(columns={'generate_pro_model_output': 'Gemini_pro_model_output'})  \n",
    "# df = df.rename(columns={'generate_pro_model_output': 'Gemini_pro_model_output'})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a32343-5ef4-404a-963d-0a95c2050849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete the 'col2' column\n",
    "df = df.drop('combine_prompt', axis=1)\n",
    "\n",
    "output1 = \"./output/O_Anaes 16 Clinical scenarios RAG response_output_gemini.csv\"\n",
    "\n",
    "df.to_csv(output1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458307ee-8043-4386-a662-db088351c1b5",
   "metadata": {},
   "source": [
    "### Second file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a6a8e-795b-4db0-8577-55c102375e2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename2 = \"./data/Anaes 16 Clinical scenarios response base.csv\"\n",
    "df = pd.read_csv(filename2)\n",
    "df['combine_prompt'] = df['System Prompts'] + ' '+ df['User Prompts']\n",
    "\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    if i<=100:\n",
    "        # df['Gemini_ultra_model_output'][i] = generate(df['combine_prompt'][i])\n",
    "        print(\"iteration #\", i)\n",
    "\n",
    "    # df.loc[i, \"Gemini_ultra_model_output\"] = generate(df.loc[i,'combine_prompt'])\n",
    "    # df.loc[i, \"Gemini_ultra_model_output_v1\"] = generate(df.loc[i,'combine_prompt'])\n",
    "    # df.loc[i, \"Gemini_ultra_model_output_v2\"] = generate(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"Gemini_pro_model_output\"] = generate_pro(df.loc[i,'combine_prompt'])\n",
    "\n",
    "    df.loc[i, \"Anthropic_haiko_model_output\"] = generate_anthropic_response(df.loc[i,'combine_prompt'])\n",
    "    \n",
    "    df.loc[i, \"medLM_model_output\"] = generate_medllms_v1(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"medpalm_model_output\"] = generate_medlpalm(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"palm_bison32k_output\"] = generate_palm_bison32k(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"palm_unicorn_output\"] = generate_palm_unicorn_v1(df.loc[i,'combine_prompt'])\n",
    "\n",
    "    # print( \"/n output here ::\" , df['Gemini_ultra_model_output'][i])\n",
    "\n",
    "df = df.drop('combine_prompt', axis=1)\n",
    "\n",
    "output2 = \"./output/O_Anaes 16 Clinical scenarios response base response_output_gemini.csv\"\n",
    "\n",
    "df.to_csv(output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e8504-e87a-49e3-a3cb-a3b6b0fc974a",
   "metadata": {},
   "source": [
    "### third file Pharmbot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff356c22-a5eb-4eaa-87b8-d2074ddf8958",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename3 = \"./data/Pharmbot 23 Clinical scenarios RAG response.csv\"\n",
    "df = pd.read_csv(filename3)\n",
    "df['combine_prompt1'] = df['System Prompts'] + ' ' +df['RAG Results 1'] + ' '+ df['User Question']\n",
    "df['combine_prompt2'] = df['System Prompts'] + ' ' +df['RAG Results 2'] + ' '+ df['User Question']\n",
    "df['combine_prompt3'] = df['System Prompts'] + ' ' +df['RAG Results 3'] + ' '+ df['User Question']\n",
    "\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    if i<=100:\n",
    "        print(\"iteration #\", i)\n",
    "\n",
    "    # df.loc[i, \"Gemini_ultra_model_output1\"] = generate(df.loc[i,'combine_prompt1'])\n",
    "    # df.loc[i, \"Gemini_ultra_model_output2\"] = generate(df.loc[i,'combine_prompt2'])\n",
    "    # df.loc[i, \"Gemini_ultra_model_output3\"] = generate(df.loc[i,'combine_prompt3'])\n",
    "\n",
    "    df.loc[i, \"Gemini_pro_model_output1\"] = generate_pro(df.loc[i,'combine_prompt1'])\n",
    "    df.loc[i, \"Gemini_pro_model_output2\"] = generate_pro(df.loc[i,'combine_prompt2'])\n",
    "    df.loc[i, \"Gemini_pro_model_output3\"] = generate_pro(df.loc[i,'combine_prompt3'])\n",
    "\n",
    "    # df.loc[i, \"medLM_model_output\"] = generate_medllms_v1(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"medpalm_model_output1\"] = generate_medlpalm(df.loc[i,'combine_prompt1'])\n",
    "    df.loc[i, \"medpalm_model_output2\"] = generate_medlpalm(df.loc[i,'combine_prompt2'])\n",
    "    df.loc[i, \"medpalm_model_output3\"] = generate_medlpalm(df.loc[i,'combine_prompt3'])\n",
    "    \n",
    "    df.loc[i, \"palm_bison32k_output1\"] = generate_palm_bison32k(df.loc[i,'combine_prompt1'])\n",
    "    df.loc[i, \"palm_bison32k_output2\"] = generate_palm_bison32k(df.loc[i,'combine_prompt2'])\n",
    "    df.loc[i, \"palm_bison32k_output3\"] = generate_palm_bison32k(df.loc[i,'combine_prompt3'])\n",
    "\n",
    "    df.loc[i, \"Anthropic_haiko_model_output1\"] = generate_anthropic_response(df.loc[i,'combine_prompt1'])\n",
    "\n",
    "    # df.loc[i, \"palm_unicorn_output\"] = generate_palm_unicorn_v1(df.loc[i,'combine_prompt'])\n",
    "    \n",
    "    # print( \"/n output here ::\" , df['Gemini_ultra_model_output'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d13087-24dd-4659-8a6f-5f03800e14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('combine_prompt1', axis=1)\n",
    "df = df.drop('combine_prompt2', axis=1)\n",
    "df = df.drop('combine_prompt3', axis=1)\n",
    "\n",
    "\n",
    "output3 = \"./output/O_Pharmbot 23 Clinical scenarios RAG response response_output_gemini.csv\"\n",
    "\n",
    "df.to_csv(output3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125e559-b8d3-4391-ad25-39469013fd1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### fourth file for Pharmbot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07ae84-abb1-431c-bbfc-047c022ca1a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename4 = \"./data/Pharmbot 23 Clinical scenarios response base.csv\"\n",
    "df = pd.read_csv(filename4)\n",
    "df['combine_prompt'] = df['System Prompts'] + ' ' + df['User Prompts']\n",
    "\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    if i<=100:\n",
    "        print(\"iteration #\", i)\n",
    "\n",
    "    # df.loc[i, \"Gemini_ultra_model_output\"] = generate(df.loc[i,'combine_prompt'])\n",
    "    # df.loc[i, \"Gemini_ultra_model_output_v1\"] = generate(df.loc[i,'combine_prompt'])\n",
    "    # df.loc[i, \"Gemini_ultra_model_output_v2\"] = generate(df.loc[i,'combine_prompt'])\n",
    "    \n",
    "    df.loc[i, \"Gemini_pro_model_output\"] = generate_pro(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"Anthropic_haiko_model_output\"] = generate_anthropic_response(df.loc[i,'combine_prompt'])\n",
    "\n",
    "    df.loc[i, \"medLM_model_output\"] = generate_medllms_v1(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"medpalm_model_output\"] = generate_medlpalm(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"palm_bison32k_output\"] = generate_palm_bison32k(df.loc[i,'combine_prompt'])\n",
    "    df.loc[i, \"palm_unicorn_output\"] = generate_palm_unicorn_v1(df.loc[i,'combine_prompt'])\n",
    "\n",
    "df = df.drop('combine_prompt', axis=1)\n",
    "\n",
    "output4 = \"./output/O_Pharmbot 23 Clinical scenarios response base response_output_gemini.csv\"\n",
    "\n",
    "df.to_csv(output4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb25d1-141f-46a2-a8b8-a6d2bb562c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b0988fd-674c-4e48-bd3c-7a6f7125f002",
   "metadata": {},
   "source": [
    "### Trying the Claude Anthropic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f512fd0-3f1a-460a-b5d0-81545996ba58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
